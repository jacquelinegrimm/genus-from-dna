{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install biopython\n\nfrom Bio import Entrez, SeqIO\nfrom sklearn.model_selection import train_test_split\nfrom transformers import TrainingArguments,Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:56:38.439439Z","iopub.execute_input":"2023-12-22T06:56:38.440418Z","iopub.status.idle":"2023-12-22T06:56:50.083032Z","shell.execute_reply.started":"2023-12-22T06:56:38.440380Z","shell.execute_reply":"2023-12-22T06:56:50.082071Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: biopython in /opt/conda/lib/python3.10/site-packages (1.81)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from biopython) (1.24.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"Entrez.email = \"jacquelinekgrimm@gmail.com\"\n\n# Get gene sequences from NCBI using accession number\ndef get_genes(accession, max_genes):\n    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n    record = SeqIO.read(handle, \"genbank\")\n    genes = {}\n    genes_count = 0\n    for feature in record.features:\n        if genes_count >= max_genes:\n            break\n        if feature.type == \"CDS\":\n            gene_name = ''\n            if 'gene' in feature.qualifiers:\n                gene_name = feature.qualifiers['gene'][0]\n            elif 'locus_tag' in feature.qualifiers:\n                gene_name = feature.qualifiers['locus_tag'][0]\n            else:\n                gene_name = f\"Unknown_gene_{len(genes)+1}\"\n            genes[gene_name] = str(feature.location.extract(record).seq)\n            genes_count += 1\n    return genes\n\n# Get Salmonella and Bacillus genes\nsalmonella_genes = get_genes(\"AL513382\", max_genes=500)\nbacillus_genes = get_genes(\"AE016877\", max_genes=500)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:52:17.593207Z","iopub.execute_input":"2023-12-22T06:52:17.593982Z","iopub.status.idle":"2023-12-22T06:53:54.208412Z","shell.execute_reply.started":"2023-12-22T06:52:17.593949Z","shell.execute_reply":"2023-12-22T06:53:54.207575Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Function to create k-mers\ndef make_kmers(seq, size):\n    return [seq[x:x + size].lower() for x in range(len(seq) - size + 1)]\n\n# Function to join k-mer words into sentences\ndef sentences(genes_dict, kmer_size):\n    gene_sentences = {}\n    for gene_name, sequence in genes_dict.items():\n        words = make_kmers(sequence, size=kmer_size)\n        joined_sentence = ' '.join(words)\n        gene_sentences[gene_name] = joined_sentence\n    return gene_sentences\n\n# Creating sentences\nsalmonella_sentences = sentences(salmonella_genes, kmer_size=6)\nbacillus_sentences = sentences(bacillus_genes, kmer_size=6)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:55:40.091720Z","iopub.execute_input":"2023-12-22T06:55:40.092662Z","iopub.status.idle":"2023-12-22T06:55:40.347190Z","shell.execute_reply.started":"2023-12-22T06:55:40.092626Z","shell.execute_reply":"2023-12-22T06:55:40.346205Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create DataFrames\nsalmonella_df = pd.DataFrame({'Sentences': list(salmonella_sentences.values()), 'Species': 0})\nbacillus_df = pd.DataFrame({'Sentences': list(bacillus_sentences.values()), 'Species': 1})\n\n# Concatenate DataFrames\ndf = pd.concat([salmonella_df, bacillus_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:57:19.925915Z","iopub.execute_input":"2023-12-22T06:57:19.926323Z","iopub.status.idle":"2023-12-22T06:57:19.946866Z","shell.execute_reply.started":"2023-12-22T06:57:19.926292Z","shell.execute_reply":"2023-12-22T06:57:19.945954Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(df['Sentences'].tolist(),\n                                                                    df['Species'].tolist(),\n                                                                    test_size=0.2,\n                                                                    random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:57:42.140972Z","iopub.execute_input":"2023-12-22T06:57:42.141318Z","iopub.status.idle":"2023-12-22T06:57:42.158485Z","shell.execute_reply.started":"2023-12-22T06:57:42.141293Z","shell.execute_reply":"2023-12-22T06:57:42.157409Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load the model and tokenizet\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the text data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:59:00.276864Z","iopub.execute_input":"2023-12-22T06:59:00.277503Z","iopub.status.idle":"2023-12-22T06:59:50.419386Z","shell.execute_reply.started":"2023-12-22T06:59:00.277469Z","shell.execute_reply":"2023-12-22T06:59:50.418564Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716deff3fe7c492592c209656bf9565f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert labels to tensors\ntrain_labels = train_labels.clone().detach()\nval_labels = val_labels.clone().detach()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:05:52.069842Z","iopub.execute_input":"2023-12-22T07:05:52.070609Z","iopub.status.idle":"2023-12-22T07:05:52.076616Z","shell.execute_reply.started":"2023-12-22T07:05:52.070576Z","shell.execute_reply":"2023-12-22T07:05:52.075466Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Convert tokenized encodings to a dataset\nclass CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = CustomDataset(train_encodings, train_labels)\nval_dataset = CustomDataset(val_encodings, val_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:06:00.287160Z","iopub.execute_input":"2023-12-22T07:06:00.287548Z","iopub.status.idle":"2023-12-22T07:06:00.296835Z","shell.execute_reply.started":"2023-12-22T07:06:00.287501Z","shell.execute_reply":"2023-12-22T07:06:00.295668Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    logging_dir='./logs',\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    load_best_model_at_end=True,\n)\n\n# Define the Trainer object\ncompute_metrics = lambda eval_pred: {'accuracy': (torch.tensor(eval_pred.predictions).argmax(-1) == torch.tensor(eval_pred.label_ids)).float().mean().item()}\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:06:04.081019Z","iopub.execute_input":"2023-12-22T07:06:04.081390Z","iopub.status.idle":"2023-12-22T07:06:04.100393Z","shell.execute_reply.started":"2023-12-22T07:06:04.081359Z","shell.execute_reply":"2023-12-22T07:06:04.099433Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:06:10.647235Z","iopub.execute_input":"2023-12-22T07:06:10.647624Z","iopub.status.idle":"2023-12-22T07:08:35.898200Z","shell.execute_reply.started":"2023-12-22T07:06:10.647592Z","shell.execute_reply":"2023-12-22T07:08:35.896434Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/785706737.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item['labels'] = torch.tensor(self.labels[idx])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 02:22, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.245062</td>\n      <td>0.890000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.236481</td>\n      <td>0.910000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.182131</td>\n      <td>0.920000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory ./results/checkpoint-25 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/tmp/ipykernel_42/785706737.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item['labels'] = torch.tensor(self.labels[idx])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/checkpoint-50 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/tmp/ipykernel_42/785706737.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item['labels'] = torch.tensor(self.labels[idx])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/checkpoint-75 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=75, training_loss=0.24059819539388022, metrics={'train_runtime': 144.4706, 'train_samples_per_second': 16.55, 'train_steps_per_second': 0.519, 'total_flos': 629098533365760.0, 'train_loss': 0.24059819539388022, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}